{
 "metadata": {
  "name": "Simple Scraping"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scraping\n",
      "=====\n",
      "\n",
      "This is a simple script that downloads files referenced as links in an HTML file. I uses this to scrape PDFs from Coursera.org course pages.\n",
      "\n",
      "(c) Florian Hoppe 2013"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urlparse\n",
      "import urllib2 as urllib\n",
      "import urllib\n",
      "import sys\n",
      "import os\n",
      "import threading\n",
      "from lxml import etree\n",
      "\n",
      "def _download_link(url,target_dir):\n",
      "    urlpath = urlparse.urlsplit(url)\n",
      "    filename = target_dir + \"/\" + urllib.unquote(urlpath[2])[1:].replace(\"/\",\"_\")\n",
      "    \n",
      "    print(\"loading \" + url + \" and saving to \" + filename)\n",
      "    if os.path.exists(filename):\n",
      "        print(filename + \" already exists. download is aborded.\")\n",
      "    else:\n",
      "        try:\n",
      "            with open(filename, \"wb\") as f:\n",
      "                req = urllib.urlopen(url)\n",
      "                f.write(req.read())\n",
      "                print(\"Done with \" + filename)\n",
      "        except IOError:\n",
      "            print(\"IO Error for \" + url)\n",
      "        \n",
      "\n",
      "def download_all( filename, link_filter = 'pdf'):\n",
      "    \"\"\"Searches the given sourcefile for links and downloads their targets.\n",
      "\n",
      "    Args:\n",
      "        filename: should reference a HMTL file in the current working directory\n",
      "        link_filter: string that contains the extension of the files that should be downloaded\n",
      "    Returns:\n",
      "        nothing. the link targets will be saved in a subdirectory (must exists before calling the function).\n",
      "    Raises:\n",
      "        nothing\n",
      "    \"\"\"\n",
      "    \n",
      "    parser = etree.HTMLParser()\n",
      "    tree = etree.parse(filename, parser)\n",
      "\n",
      "    links = tree.xpath('//a')\n",
      "    \n",
      "    for link in links:\n",
      "        if 'href' in link.attrib:\n",
      "            url = link.attrib['href']\n",
      "\n",
      "            if url[-3:] == link_filter:\n",
      "                t = threading.Thread(target=_download_link,args=(url,link_filter + 's'))\n",
      "                t.start()\n",
      "#                download_link(url,link_filter + 's')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /Users/florianhoppe/Documents/SkyDrive/Coding/python/scrapy_tutorial"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/florianhoppe/Documents/SkyDrive/Coding/python/scrapy_tutorial\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd /Users/florianhoppe/Google Drive/Wissen/MOOCs/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/florianhoppe/Google Drive/Wissen/MOOCs\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mkdir pdfs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "download_all('Data Analysis.html')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}